# -*- coding: utf-8 -*-
"""new weibull code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ob_Q_X_viekuiwUplyUQMUvg7kLg2v_H
"""

# gru_weibull_attention

import os
import numpy as np
import pandas as pd
from scipy import stats
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from scipy.stats import ttest_rel
import random

# -----------------------------
# Config
# -----------------------------
CSV_PATH = "/content/drive/MyDrive/Book Chapter-Weibull/Dataset -DNP3.csv"
OUT_CSV = "./weibull_attention.csv"
SEQ_LEN = 10                # sliding window length
STRIDE = 1
BATCH_SIZE = 64
EPOCHS = 16
LR = 1e-3
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
SEED = 42
NUM_RUNS = 5                # number of repeated experiments for t-test
LAMBDA_INIT = 0.5           # initial lambda for GRUWA (learnable, constrained positive)
# -----------------------------
# Reproducibility
# -----------------------------
def set_seed(seed=SEED):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

set_seed()

# -----------------------------
# 1) Load CSV and select numeric features
# -----------------------------
df = pd.read_csv(CSV_PATH)
# keep original index
df = df.reset_index(drop=True)

# choose numeric columns
num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
if len(num_cols) == 0:
    raise RuntimeError("No numeric columns detected in the CSV. Provide numeric features.")

# We'll use all numeric columns as features
features = df[num_cols].copy()

# -----------------------------
# 2) Compute per-row magnitude (L2 norm) and fit Weibull
# -----------------------------
# Compute L2 norm per row
row_norms = np.linalg.norm(features.values, axis=1)

# Fit Weibull (we use scipy.stats.weibull_min)
# weibull_min parameterization: c = shape, scale = scale
# We fit on the norms (positive values)
c, loc, scale = stats.weibull_min.fit(row_norms, floc=0)  # fix loc=0 for stability

# Compute Weibull survival function: sf(x) = 1 - cdf(x)
weibull_scores = stats.weibull_min.sf(row_norms, c, loc=loc, scale=scale)
# smaller weibull_scores indicate tail/anomalous points (as wanted)

df['weibull_score'] = weibull_scores

# -----------------------------
# 3) Compute proxy attention scores from a_t only
#    using alpha_t \propto exp(-lambda * a_t)
#    This yields higher attention when a_t is smaller.
# -----------------------------
lambda_proxy = 2.0
# We'll compute attention per sliding window later; but to create a per-row attention
# we compute a local softmax across a small neighbourhood (window radius)
# For a global per-row proxy (simple): softmax(-lambda * a_t) across the whole dataset
logits_proxy = -lambda_proxy * df['weibull_score'].values
# stable softmax
logits_proxy = logits_proxy - np.max(logits_proxy)
exp_logits = np.exp(logits_proxy)
attention_proxy = exp_logits / np.sum(exp_logits)
df['attention_score'] = attention_proxy

# -----------------------------
# 4) Create risk categories from attention_score thresholds
#    less than 0.1 -> low
#    0.1 to 0.7 -> medium
#    0.7 to 0.8 -> high
#    > 0.8 -> critical
# -----------------------------
def attention_to_risk(a):
    if a < 0.1:
        return "low"
    elif a < 0.7:
        return "medium"
    elif a < 0.8:
        return "high"
    else:
        return "critical"

df['risk'] = df['attention_score'].apply(attention_to_risk)

# Save augmented file
df.to_csv(OUT_CSV, index=False)
print(f"Augmented CSV saved to: {OUT_CSV}")
print(df[['weibull_score','attention_score','risk']].head())

# -----------------------------
# 5) Build sequences (sliding windows) for model training
#    Each sample: sequence of feature vectors (shape: SEQ_LEN x num_features)
#    Label = risk of the last timestep in the window.
# -----------------------------
X = features.values
y_risk = df['risk'].values

def build_sequences(X, y, seq_len=SEQ_LEN, stride=STRIDE):
    Xs, Ys = [], []
    N = X.shape[0]
    for start in range(0, N - seq_len + 1, stride):
        end = start + seq_len
        Xs.append(X[start:end])
        Ys.append(y[end-1])  # label is risk at last timestep
    return np.stack(Xs), np.array(Ys)

X_seq, y_seq = build_sequences(X, y_risk, SEQ_LEN, STRIDE)
print("Sequences built:", X_seq.shape, y_seq.shape)

# Encode labels
le = LabelEncoder()
y_enc = le.fit_transform(y_seq)  # yields ints 0..K-1
num_classes = len(le.classes_)
print("Classes:", list(le.classes_))

# Standardize features across dataset (fit on all sequences flattened)
scaler = StandardScaler()
flat = X_seq.reshape(-1, X_seq.shape[-1])
scaler.fit(flat)
X_seq_scaled = scaler.transform(flat).reshape(X_seq.shape)

# -----------------------------
# 6) PyTorch Dataset
# -----------------------------
class SequenceDataset(Dataset):
    def __init__(self, X, y):
        # X: (N, seq_len, feat_dim)
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.long)
    def __len__(self):
        return len(self.X)
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# -----------------------------
# 7) Models
# -----------------------------
class GRUClassifier(nn.Module):
    def __init__(self, in_dim, hid_dim, num_layers, num_classes, dropout=0.1):
        super().__init__()
        self.gru = nn.GRU(in_dim, hid_dim, num_layers=num_layers, batch_first=True, bidirectional=False)
        self.pool = nn.AdaptiveAvgPool1d(1)  # mean pool over time
        self.fc = nn.Sequential(
            nn.Linear(hid_dim, hid_dim//2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hid_dim//2, num_classes)
        )
    def forward(self, x):
        # x: B x T x F
        out, _ = self.gru(x)            # out: B x T x H
        # mean over time
        feat = out.mean(dim=1)          # B x H
        logits = self.fc(feat)
        return logits

class GRUWithWeibullAttention(nn.Module):
    def __init__(self, in_dim, hid_dim, num_layers, num_classes, attn_dim=None, dropout=0.1):
        super().__init__()
        self.gru = nn.GRU(in_dim, hid_dim, num_layers=num_layers, batch_first=True)
        self.attn_W = nn.Linear(hid_dim, hid_dim)   # W_h
        self.attn_v = nn.Linear(hid_dim, 1, bias=False)  # v^T (projects to scalar)
        # lambda parameter (constrained positive by softplus)
        self.lambda_param = nn.Parameter(torch.tensor(LAMBDA_INIT))
        self.fc = nn.Sequential(
            nn.Linear(hid_dim, hid_dim//2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hid_dim//2, num_classes)
        )
    def forward(self, x, weibull_scores_seq):
        # x: B x T x F
        # weibull_scores_seq: B x T (torch tensor)
        out, _ = self.gru(x)        # B x T x H
        # compute e_t = v^T tanh(W_h h_t + b) => use attn_W and attn_v
        e = torch.tanh(self.attn_W(out))        # B x T x H
        e = self.attn_v(e).squeeze(-1)          # B x T (scalar scores)
        # apply lambda (ensure positive): softplus to constrain >=0
        lambda_pos = torch.nn.functional.softplus(self.lambda_param)
        logits = e - lambda_pos * weibull_scores_seq  # B x T
        # softmax over time
        attn_weights = torch.softmax(logits, dim=1)   # B x T
        # context vector
        context = torch.sum(attn_weights.unsqueeze(-1) * out, dim=1)  # B x H
        logits_out = self.fc(context)  # B x C
        return logits_out, attn_weights

# -----------------------------
# 8) Training / eval utilities
# -----------------------------
def train_epoch(model, loader, opt, criterion, device, is_gruwa=False, weibull_seq_tensor=None):
    model.train()
    total_loss = 0.0
    correct = 0
    total = 0
    for Xb, yb in loader:
        Xb = Xb.to(device)
        yb = yb.to(device)
        if is_gruwa:
            # need to compute per-sample weibull sequences (B x T)
            # caller will provide a function to fetch them; we compute them here from global array
            # but for simplicity, we will compute based on precomputed df attention mapping
            # Instead we will pass weibull_seq_tensor as a global tensor aligned to dataset
            pass
        opt.zero_grad()
        if is_gruwa:
            # we assume we have a pre-aligned tensor `weibull_for_dataset` of shape (N, T)
            logits, _ = model(Xb, weibull_for_dataset[batch_indices_ptr: batch_indices_ptr + len(Xb)].to(device))
        else:
            logits = model(Xb)
        loss = criterion(logits, yb)
        loss.backward()
        opt.step()
        total_loss += loss.item() * len(Xb)
        preds = logits.argmax(dim=1)
        correct += (preds == yb).sum().item()
        total += len(Xb)
    return total_loss / total, correct / total

def eval_model(model, loader, device, is_gruwa=False):
    model.eval()
    correct = 0
    total = 0
    all_preds = []
    with torch.no_grad():
        for Xb, yb in loader:
            Xb = Xb.to(device)
            yb = yb.to(device)
            if is_gruwa:
                logits, _ = model(Xb, weibull_for_dataset[batch_indices_ptr: batch_indices_ptr + len(Xb)].to(device))
            else:
                logits = model(Xb)
            preds = logits.argmax(dim=1)
            correct += (preds == yb).sum().item()
            total += len(Xb)
            all_preds.append(preds.cpu().numpy())
    acc = correct / total
    return acc

# NOTE: The above train_epoch/eval_model functions rely on having per-sample weibull sequences
# pre-aligned for the DataLoader batches. To keep the code clean and robust without fancy
# DataLoader index tricks, we'll implement training loops manually below where we can control
# batch indices and therefore slice weibull_for_dataset accordingly.

# -----------------------------
# 9) Prepare weibull sequences aligned with X_seq
# -----------------------------
# For each sequence X_seq[i] (which spans rows start..end), we create weibull vector
# by looking up df['weibull_score'] for the corresponding rows. We built sequences using simple sliding windows.
def build_weibull_sequences(weibull_scores_array, seq_len=SEQ_LEN, stride=STRIDE):
    seqs = []
    N = len(weibull_scores_array)
    for start in range(0, N - seq_len + 1, stride):
        end = start + seq_len
        seqs.append(weibull_scores_array[start:end])
    return np.stack(seqs)  # (n_samples, seq_len)

weibull_seq = build_weibull_sequences(df['weibull_score'].values, SEQ_LEN, STRIDE)
print("Weibull seq shape:", weibull_seq.shape)

# -----------------------------
# 10) Split dataset into train/test
# -----------------------------
X_train, X_test, w_train, w_test, y_train, y_test = train_test_split(
    X_seq_scaled, weibull_seq, y_enc, test_size=0.2, random_state=SEED, stratify=y_enc
)

train_dataset = SequenceDataset(X_train, y_train)
test_dataset = SequenceDataset(X_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

# For aligning weibull sequences to batch slices during manual training:
weibull_for_dataset_train = torch.tensor(w_train, dtype=torch.float32).to(DEVICE)
weibull_for_dataset_test = torch.tensor(w_test, dtype=torch.float32).to(DEVICE)

# -----------------------------
# 11) Training loops (manual batching to pass weibull sequences correctly)
# -----------------------------
def train_and_eval_gru(in_dim, hid_dim=64, num_layers=1):
    set_seed()
    model = GRUClassifier(in_dim, hid_dim, num_layers, num_classes).to(DEVICE)
    opt = torch.optim.Adam(model.parameters(), lr=LR)
    criterion = nn.CrossEntropyLoss()
    # simple train
    for epoch in range(EPOCHS):
        # train
        model.train()
        total_loss = 0.0
        total = 0
        correct = 0
        # manual batching over training tensor
        idxs = np.arange(len(X_train))
        np.random.shuffle(idxs)
        for i in range(0, len(idxs), BATCH_SIZE):
            batch_idx = idxs[i:i+BATCH_SIZE]
            Xb = torch.tensor(X_train[batch_idx], dtype=torch.float32).to(DEVICE)
            yb = torch.tensor(y_train[batch_idx], dtype=torch.long).to(DEVICE)
            opt.zero_grad()
            logits = model(Xb)
            loss = criterion(logits, yb)
            loss.backward()
            opt.step()
            total_loss += loss.item() * Xb.size(0)
            preds = logits.argmax(dim=1)
            correct += (preds == yb).sum().item()
            total += Xb.size(0)
        #print(f"Epoch {epoch+1}/{EPOCHS} train loss {total_loss/total:.4f} acc {correct/total:.4f}")
    # eval
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for i in range(0, len(X_test), BATCH_SIZE):
            batch_idx = np.arange(i, min(i+BATCH_SIZE, len(X_test)))
            Xb = torch.tensor(X_test[batch_idx], dtype=torch.float32).to(DEVICE)
            yb = torch.tensor(y_test[batch_idx], dtype=torch.long).to(DEVICE)
            logits = model(Xb)
            preds = logits.argmax(dim=1)
            correct += (preds == yb).sum().item()
            total += Xb.size(0)
    return correct/total

def train_and_eval_gruwa(in_dim, hid_dim=64, num_layers=1):
    set_seed()
    model = GRUWithWeibullAttention(in_dim, hid_dim, num_layers, num_classes).to(DEVICE)
    opt = torch.optim.Adam(model.parameters(), lr=LR)
    criterion = nn.CrossEntropyLoss()
    # train
    n_train = len(X_train)
    for epoch in range(EPOCHS):
        idxs = np.arange(n_train)
        np.random.shuffle(idxs)
        for i in range(0, n_train, BATCH_SIZE):
            batch_idx = idxs[i:i+BATCH_SIZE]
            Xb = torch.tensor(X_train[batch_idx], dtype=torch.float32).to(DEVICE)
            yb = torch.tensor(y_train[batch_idx], dtype=torch.long).to(DEVICE)
            wb = torch.tensor(w_train[batch_idx], dtype=torch.float32).to(DEVICE)
            opt.zero_grad()
            logits, _ = model(Xb, wb)
            loss = criterion(logits, yb)
            loss.backward()
            opt.step()
    # eval
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for i in range(0, len(X_test), BATCH_SIZE):
            batch_idx = np.arange(i, min(i+BATCH_SIZE, len(X_test)))
            Xb = torch.tensor(X_test[batch_idx], dtype=torch.float32).to(DEVICE)
            yb = torch.tensor(y_test[batch_idx], dtype=torch.long).to(DEVICE)
            wb = torch.tensor(w_test[batch_idx], dtype=torch.float32).to(DEVICE)
            logits, _ = model(Xb, wb)
            preds = logits.argmax(dim=1)
            correct += (preds == yb).sum().item()
            total += Xb.size(0)
    return correct/total

# -----------------------------
# 12) Run multiple experiments to compare GRU vs GRUWA and do paired t-test
# -----------------------------
in_dim = X_seq.shape[-1]
gru_accs = []
gruwa_accs = []
for run in range(NUM_RUNS):
    print(f"Run {run+1}/{NUM_RUNS} ...")
    acc_gru = train_and_eval_gru(in_dim)
    acc_gruwa = train_and_eval_gruwa(in_dim)
    print(f"  GRU acc:   {acc_gru:.4f}")
    print(f"  GRUWA acc: {acc_gruwa:.4f}")
    gru_accs.append(acc_gru)
    gruwa_accs.append(acc_gruwa)

gru_accs = np.array(gru_accs)
gruwa_accs = np.array(gruwa_accs)

mean_gru, std_gru = gru_accs.mean(), gru_accs.std(ddof=1)
mean_gruwa, std_gruwa = gruwa_accs.mean(), gruwa_accs.std(ddof=1)

print("\nSummary of runs:")
print(f"GRU    mean acc = {mean_gru:.4f} ± {std_gru:.4f}")
print(f"GRUWA  mean acc = {mean_gruwa:.4f} ± {std_gruwa:.4f}")

# paired t-test
t_stat, p_val = ttest_rel(gruwa_accs, gru_accs)
print(f"Paired t-test (GRUWA vs GRU): t={t_stat:.4f}, p={p_val:.6f}")

# final message for significance (no fabrication — report actual p-value)
if p_val < 0.01:
    print("Result: difference is statistically significant at p < 0.01.")
if p_val < 0.005:
    print("Result: GRUWA achieved larger improvements with p < 0.005.")

# Save accuracies (optional)
np.savez("accuracies_gru_vs_gruwa.npz", gru_accs=gru_accs, gruwa_accs=gruwa_accs)
print("Done.")